# 第1章 关于本书的对话

三个关键概念：

- virtualization（虚拟化）
- concurrency（并发）
- persistence（持久性）

# 第2章 操作系统介绍

推荐书籍：

- 《Introduction to Computing Systems: From Bits and Gates to C and Beyond》Yale N. Patt and Sanjay J. Patel McGraw-Hill, 2003

- 《Computer Systems: A Programmer’s Perspective》 Randal E. Bryant and David R. O’Hallaron
  Addison-Wesley, 2010

程序运行时会发生什么？执行指令

处理器从内存中获取（fetch）一条指令，对其进行解码（decode）（弄清楚这是哪条指令），然后执行（execute）它（做它应该做的事情，如两个数相加、访问内存、检查条件、跳转到函数等）。完成这条指令后，处理器继续执行下一条指令，依此类推，直到程序最终完成。

如何将资源虚拟化

操作系统主要利用虚拟化（virtualization）技术，将物理（physical）资源（如处理器、内存或磁盘）转换为更通用、更强大且更易于使用的虚拟形式。有时将操作系统称为虚拟机（virtual machine）。

操作系统还提供了一些接口（API）、系统调用（system call）。由于操作系统提供这些调用来运行程序、访问内存和设备，并进行其他相关操作，有时也会说操作系统为应用程序提供了一个标准库（standard library）。

虚拟化让许多程序运行（从而共享CPU），让许多程序可以同时访问自己的指令和数据（从而共享内存），让许多程序访问设备（从而共享磁盘等），所以操作系统有时被称为资源管理器（resource manager）。

# 第4章 抽象：进程

## 4.1 抽象：进程

操作系统为正在运行的程序提供的抽象，就是进程（process）。

## 4.4 进程状态

- 运行（running）：在运行状态下，进程正在处理器上运行。这意味着它正在执行指令。

- 就绪（ready）：在就绪状态下，进程已准备好运行，但由于某种原因，操作系统选择不在此时运行。

- 阻塞（blocked）：在阻塞状态下，一个进程执行了某种操作，直到发生其他事件时才会准备运行。

# 第5章 插叙：进程API

- fork()

- exec()

- wait()

# 第6章 机制：受限直接执行

## 6.3 问题2：在进程之间切换

- 时钟中断（timer interrupt）

- 上下文切换（context switch）

# 第7章 进程调度：介绍

## 7.3 先进先出（FIFO）

先进先出（First In First Out或FIFO），也称先到先服务（First Come First Served或FCFS）

## 7.4 最短任务优先（SJF）

最短任务优先（Shortest Job First，SJF）

## 7.5 最短完成时间优先（STCF）

最短完成时间优先（Shortest Time-to-Completion First，STCF）或抢占式最短作业优先（Preemptive Shortest Job First，PSJF）

## 7.7 轮转

轮转（Round-Robin，RR）。基本思想：RR在一个时间片（time slice）内运行一个工作，然后切换到运行队列中的下一个任务，而不是运行一个任务直到结束。它反复执行，直到所有任务完成。时间片长度必须是时钟中断周期的倍数。

# 第8章 调度：多级反馈队列

多级反馈队列（Multi-level Feedback Queue，MLFQ）

## 8.1 MLFQ：基本规则

MLFQ中有许多独立的队列，每个队列有不同的优先级（priority level）。任何时刻，一个工作只能存在于一个队列中。MLFQ总是优先执行较高优先级的工作。

每个队列中可能会有多个工作，因此具有同样的优先级。在这种情况下，我们就对这些工作采用轮转调度。

MLFQ两条基本规则：

- 规则1：如果A的优先级大于B的优先级，运行A。

- 规则2：如果A的优先级等于B的优先级，轮转运行A和B。

## 8.2 尝试1：如何改变优先级

- 规则3：工作进入系统时，放在最高优先级。

- 规则4a：工作用完整个时间片后，降低其优先级。

- 规则4b：如果工作在其时间片以内主动释放CPU，则优先级不变。

## 8.3 尝试2：提升优先级

- 规则5：经过一段时间S，就将系统中所有工作重新加入最高优先级队列。

## 8.4 尝试3：更好的计时方式

- 规则4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次CPU），就降低其优先级。

# 第9章 调度：比例份额

比例份额（proportional-share），有时也称为公平份额（fair-share）。比例份额算法基于一个简单的想法：调度程序的最终目标，是确保每个工作获得一定比例的CPU时间，而不是优化周转时间和响应时间。

彩票调度（lottery scheduling），基本思想：每个一段时间，都会举行一次彩票抽奖，以确定接下来应该运行哪个进程。

## 9.1 基本概念：彩票数表示份额

彩票数（ticket）代表了进程占有某个资源的份额。一个进程拥有彩票数占总彩票数的百分比，就是它占有资源的份额。

# 第10章 多处理器调度（高级）

## 10.1 背景：多处理器架构

缓存是基于局部性（locality）的概念，局部性有两种，即时间局部性和空间局部性。时间局部性是指当一个数据被访问后，它很有可能会在不久的将来被再次访问。而空间局部性指的是，当程序访问地址为x的数据时，很有可能会紧接着访问x周围的数据。

## 10.4 单队列调度

单队列多处理器调度（Single Queue Multiprocessor Scheduling，SQMS）。

缺点：

- 缺乏可扩展性（scalability）为了保证在多CPU上正常运行，调度程序的开发者需要在代码中通过加锁来保证原子性。锁可能带来巨大的性能损失。

- 第二个问题是不能很好的保证缓存亲和性

## 10.5 多队列调度

多队列多处理器调度（Multi-Queue Multiprocessor Scheduling，MQMS）。基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则。

MQMS比SQMS有明显优势，它天生更具有扩展性。队列的数量会随着CPU的增加而增加，因此锁和缓存争用的开销不是大问题。MQMS天生具有良好的缓存亲和度。所有工作都保持在固定的CPU上，因而可以很好地利用缓存数据。

缺点：负载不均衡。

让工作移动，这种技术称为迁移（migration）。通过工作的跨CPU迁移，可以真正实现负载均衡。

工作窃取（work stealing）

# 第13章 抽象：地址空间

虚拟内存：虚拟内存系统负责为程序提供一个巨大的、稀疏的、私有的地址空间的假象，其中保存了程序的所有指令和数据。操作系统在专门硬件的帮助下，通过每一个虚拟内存的索引，将其转换为物理地址，物理内存根据获得的物理地址去获取所需的信息。操作系统会同时对许多进程执行此操作，并且确保程序之间互相不会受到影响，也不会影响操作系统。

# 第14章 插叙：内存操作API

## 14.1 内存类型

运行C程序的时候，会分配两种类型的内存：

- 栈内存，它的申请和释放操作是编译器来隐式管理的，也称为自动（automatic）内存

- 堆内存，所有的申请和释放操作都由程序员显式的完成。

API：

- malloc()

- free()

- mmap()

- calloc()

- realloc()

# 第15章 机制：地址转换

基于硬件的地址转换（hardware-based address translation）简称地址转换（address translation）。利用地址转换，硬件对每次内存访问进行处理，将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址。

## 15.3 动态（基于硬件）重定位

基址加界限机制（base and bound），又称动态重定位（dynamic relocation）。

每个CPU需要两个硬件寄存器：基址（base）寄存器和界限（bound）寄存器，有时称为限制（limit）寄存器。

将虚拟地址转换为物理地址，就是地址转换（address translation）技术。这种重定位是在运行时发生的，这种技术一般被称为动态重定位（dynamic relocation）

基址寄存器配合界限寄存器的硬件结构是芯片中的（每个CPU一对）。将CPU的这个负责地址转换的部分通称为内存管理单元（Memory Management Unit，MMU）。

内部碎片（internal fragmentation），指的是已经分配的内存单元内部有未使用的空间（即碎片），造成了浪费。

# 第16章 分段

## 16.1 分段：泛化的基址/界限

分段（segmentation）概念。在MMU中引入不止一个基址和界限寄存器对，给地址空间内的每个逻辑段（segment）一对。一个段只是地址空间里的一个连续定长的区域，在典型的地址空间里有3个逻辑不同的段：代码、栈、堆。分段的机制使得操作系统能够将不同的段放到不同的物理内存区域，从而避免了虚拟地址空间中的未使用部分占用物理内存。

## 16.2 我们引用哪个段

硬件在地址转换时使用段寄存器。它如何知道段内的偏移量，以及地址引用了哪个段？

显式的方式：用虚拟地址的开头几位来标识不同的段，剩余的位来表示段内偏移。

隐式的方式：硬件通过地址产生的方式来确定段。例如，如果地址由程序计数器产生，那么地址在代码段。如果基于栈或基址指针，它一定在栈段。其他地址则在堆段。

## 16.4 支持共享

为了支持共享，需要一些额外的硬件支持，这就是保护位（protection bit）。基本为每个段增加了几个位，标识程序是否能够读写该段，或执行其中的代码。

## 16.6 操作系统支持

外部碎片（external fragmentation），物理内存很快充满了许多空闲空间的小洞，因而很难分配给新的段，或扩大已有的段。

该问题的解决方案：

- 一种是紧凑（compact）物理内存，重新安排原有的段。操作系统先终止运行的进程，将它们的数据复制到连续的内存区域中去，改变它们的段寄存器中的值，指向新的物理地址，从而得到了足够大的连续空闲空间。这样做，操作系统能让新的内存分配请求成功。但是，内存紧凑成本很高，因为拷贝段是内存密集型的，一般占用大量的处理器时间。

- 另外一种做法是利用空闲列表管理算法，试图保留大的内存块用于分配。相关算法有很多：
  
  - 最优匹配（best-fit）
  
  - 最坏匹配（worst-fit）
  
  - 首次匹配（first-fit）
  
  - 伙伴算法（buddy algorithm）

# 第17章 空闲空间管理

## 17.3 基本策略

### 最优匹配

最优匹配（best fit）策略非常简单：首先遍历整个空闲列表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块。也可以称为最小匹配。只需要遍历一次空闲列表，就足以找到正确的块并返回。

简单的实现在遍历查找正确的空闲块时，要付出较高的性能代价。

### 最差匹配

最差匹配（worst fit）方法与最优匹配相反，它尝试找最大的空闲块，分割并满足用户需求后，将剩余的块加入空闲列表。最差匹配尝试在空闲列表中保留较大的块，而不是像最优匹配那样可能剩下很多难以利用的小块。

但是最差匹配同样需要遍历整个空闲列表。还会导致过量的碎片。

### 首次匹配

首次匹配（first fit）策略就是找到第一个足够大的块，将请求的空间返回给用户。剩余的空闲空间留给后续请求。

首次匹配有速度优势，不需要遍历所有空闲块，但是有时会让空闲列表开头的部分有很多小块。

### 下次匹配

下次匹配（next fit）算法多维护一个指针，指向上一次查找结束的位置。不像首次匹配那样每次都从列表的开始查找。将对空闲空间的查找扩散到整个列表中去，避免对列表开头频繁的分割。

这种策略的性能与首次匹配很接近，同样避免了遍历查找。

## 17.4 其他方式

### 分离空闲列表

分离空闲列表（segregated list），基本想法：如果某个应用程序经常申请一种（或几种）大小的内存空间，那就用一个独立的列表，只管理这样大小的对象。其他大小的请求都交给更通用的内存分配程序。

Solaris系统内核设计的厚块分配程序（slab allocator）：内核启动时，它为可能频繁请求的内核对象创建一些对象缓存（object cache），如锁和文件系统inode等。这些对象缓存分别属于不同大小的空闲列表，因此能够很快的响应内存请求和释放。如果某个缓存中的空闲空间快耗尽时，它就向通用内存分配程序申请一些内存厚块（slab）。相反，如果给定厚块中对象的引用计数为0,通用内存分配程序可以从专门分配程序中回收这些空间。

### 伙伴系统

二分伙伴分配程序（binary buddy allocator），在这种系统中，空闲空间首先从概念上被砍成大小为 $2^N$的大空间。当有一个内存分配请求时，空闲空间被递归的一分为二，直到刚好可以满足请求的大小。这时，请求的块被返回给用户。

这种策略只需允许分配2的整数次幂大小的空闲块，因此会有内部碎片的麻烦。

在块被释放时，分配程序会检查伙伴是否空闲，如果是就合二为一，会递归进行合并，知道合并整个内存区域或某一块的伙伴还没有被释放。

# 第18章 分页：介绍

有两种方法来解决大多数空间管理问题：

- 第一种是将空间分割成不同长度的分片，就像虚拟内存管理中的分段。将空间切成不同长度分片以后，空间本身会碎片化（fragmented），随着时间推移，分配内存会变得比较困难。

- 第二种是将空间分割成固定长度的分片。称为分页。分页不是将一个进程的地址空间分割成几个不同长度的逻辑段（即代码、堆、栈），而是分割成固定大小的单元，每个单元称为一页。把物理内存看成是定长槽块的阵列，叫做页帧（page frame）。每个这样的页帧包含一个虚拟内存页。

## 18.1 一个简单例子

为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保存一个数据结构，称为页表（page table）。

页表的主要作用是为地址空间的每个虚拟页面保存地址转换（address translation），从而让我们知道每个页在物理内存中的位置。

虚拟地址，分成两个组件：虚拟页面号（virtual page number，VPN）和页内的偏移量（offset）。

物理帧号（PFN）也称物理页号（physical page number，PPN）

## 18.3 页表中究竟有什么

页表就是一种数据结构，用于将虚拟地址（虚拟页号）映射到物理地址（物理帧号）。

最简单的形式称为线性页表（linear page table），就是一个数组。操作系统通过虚拟页号（VPN）检索该数组，并在该索引处查找页表项（PTE），以便找到期望的物理帧号（PFN）。

页表项（PTE）有许多不同的位：

- 有效位（valid bit），通常用于指示特定地址转换是否有效。例如，当一个程序开始运行时，它的代码和堆在其地址空间的一端，栈在另一端。所有未使用的中间空间都将被标记为无效（invalid）。通过将地址空间中所有未使用的页面标记为无效，不再需要为这些页面分配物理帧，从而节省大量内存。

- 保护位（protection bit），表明页是否可以读取、写入或执行。

- 存在位（present bit），表示该页是在物理存储器还是磁盘上（即它已被换出，swapped out）。

- 脏位（dirty bit），表明页面被带入内存后是否被修改过。

- 参考位（reference bit，也称访问位，accessed bit），有时用于追踪页是否被访问，也用于确定哪些页很受欢迎，应该保留在内存中。

# 第19章 分页：快速地址转换（TLB）

使用分页作为核心机制来实现虚拟内存，可能会带来较高的性能开销。因为要使用分页，就要将内存地址空间切分成大量固定大小的单元（页），并且需要记录这些单元的地址映射信息。因为这些映射信息一般存储在物理内存中，所以在转换虚拟地址时，分页逻辑上需要一次额外的内存访问。每次指令获取、显式加载或保存，都要额外读一次内存以得到转换信息，这慢的无法接受。

地址转换旁路缓冲存储器（translation-lookaside buffer，TLB），它是虚拟到物理地址转换的硬件缓存（cache）。对每次内存访问，硬件先检查TLB,看其中是否有期望的转换映射，如果有，就完成转换，不用访问页表。

## 19.4 TLB的内容

TLB项内容可能像下面这样：`VPN | PFN | 其他位`，VPN和PFN同时存在与TLB中，因为一条地址映射可能出现在任意位置（用硬件的术语，TLB被称为全相联的（fully-associative）缓存）。硬件并行的查找这些项，看看是否有匹配。

其他位包含：

- 有效（valid）位，用来标识该项是不是有效地址转换映射

- 保护（protection）位，用来标识该页是否具有访问权限。例如，代码页被标识为可读和可执行，而堆的页被标识为可读和可写。

- 地址空间标识符（address-space identifier）

- 脏位（dirty bit）

## 19.5 上下文切换时对TLB的处理

进程切换时如何管理TLB的内容，可能的方案有：

一种方法是在上下文切换时，简单的清空TLB，把全部有效位置为0。如果是软件管理TLB的系统，可以在发生上下文切换时，通过一条显式（特权）指令来完成；如果是硬件管理TLB，可以在页表基址寄存器内容发生变化时清空TLB。

上下文切换的时候清空TLB，会有一定开销，每次进程运行都会触发TLB未命中，如果频繁的切换进程，开销很高。

为了减少这种开销，一些系统增加了硬件支持，实现跨上下文切换的TLB共享。比如在TLB中添加了一个地址空间标识符（Address Space Identifier，ASID）。可以把ASID看成是进程标识符PID。有了地址空间标识符，TLB可以同时缓存不同进程的地址空间映射。

## 19.6 TLB替换策略

- 最近最少使用（least-recently-used，LRU）

- 随机（random）

# 第20章 分页：较小的表

## 20.1 简单的解决方案：更大的页

可以使用更大的页来减少页表的大小。

这种方法主要问题在于，大内存页会导致每页内的浪费，也就是内部碎片（internal fragmentation）。因此大多数系统在常见情况下使用相对较小的页大小：4KB或8KB

## 20.2 混合方法：分页和分段

将分页和分段结合。这种方法不是为进程的整个地址空间提供单个页表，而是为每个逻辑分段提供一个。这种方式和线性页表相比，显著的节省了内存。但是可能会导致大量的页表浪费，可能会导致外部碎片出现。

## 20.3 多级页表

多级页表（multi-level page table）。思想很简单，首先将页表分成页大小的单元，然后如果整页的页表项（PTE）无效，就完全不分配该页的页表。为了追踪页表的页是否有效，使用了名为页目录（page directory）的新结构。页目录可以告诉你页表的页在哪里，或者页表的整个页不包含有效页。

多级页表在TLB未命中时，需要从内存加载两次，才能从页表中获取正确的地址转换信息。

## 20.4 反向页表

反向页表（inverted page table），这里只保留了一个页表，其中的项代表系统的每个物理页。页表项告诉我们哪个进程正在使用此项，以及该进程的哪个虚拟页映射到此物理页。

要找到正确的项，就要搜索这个数据结构，线性扫描是昂贵的，因此通常在此基础结构上建立散列表，以加速查找。

# 第21章 超越物理内存：机制

## 21.1 交换空间

交换空间（swap space），在硬盘上开辟一部分空间用于物理页的移入和移出。

## 21.2 存在位

硬件（或操作系统）判断页是否在内存中的方法，是通过页表项中的一条新信息，即存在位（present bit）。如果存在位设置为1，则表示该页存在于物理内存中；如果存在位设置为零，则页不再内存中，而在硬盘上。

访问不在物理内存中的页，这种行为通常被称为页错误（page fault）

## 21.3 页错误

当I/O在运行时，进程将处于阻塞（blocked）状态。

## 21.4 内存满了怎么办

页交换策略（page-replacement policy）

# 第22章 超越物理内存：策略

## 22.2 最优替换策略

替换内存中在最远将来才会被访问到的页，可以达到缓存为命中率最低。

## 22.3 简单策略：FIFO

FIFO先入先出替换策略

## 22.4 另一简单策略：随机

随机替换策略

## 22.5 利用历史数据：LRU

页替换策略可以使用的一个历史信息是频率（frequency）。如果一个页被访问了很多次，也许它不应该被替换。页更常用的属性是访问的近期性（recency），越近被访问过的页，也许再次访问的可能性也就越大。

## 22.8 近似LRU

需要硬件增加一个使用位（use bit）有时称为引用位（reference bit）。系统的每个页有一个使用位，然后这些使用位存在某个地方。每当页被引用时，硬件将使用位设置为1。

时钟算法（clock algorithm）、

# 第26章 并发：介绍

## 26.3 核心问题：不可控的调度

竞态条件（race condition）

临界区（critical section），是访问共享变量（共享资源）的代码片段，一定不能由多个线程同时执行。

互斥（mutual exclusion），这个属性保证了如果一个线程在临界区内执行，其他线程将被阻止进入临界区。

## 26.4 原子性愿望

要求硬件提供一些有用的指令，可以在这些指令上构建一个通用的集合，即所谓的同步原语（synchronization primitive）。

# 第28章 锁

我们希望原子式执行一系列指令，但由于单处理器上的中断（或多线程在多处理器上并发执行），我们做不到。锁（lock）可以直接解决这一问题。

## 28.1 锁的基本思想

锁就是一个变量。这个锁变量保存了锁在某一时刻的状态。它要么是可用的，表示没有线程持有锁，要么是被占用的，表示有一个线程持有锁，正处于临界区。也可以保存其他的信息，比如持有锁的线程，或请求获取锁的线程队列。

## 28.2 Pthread锁

POSIX库将锁称为互斥量（mutex），因为它被用来提供线程之间的互斥。即当一个线程在临界区，它能够阻止其他线程进入直到本线程离开临界区。

## 28.4 评价锁

- 提供互斥（mutual exclusion）：最基本的，锁是否有效，能够阻止多个线程进入临界区

- 公平性（fairness）：当锁可用时，是否每一个竞争线程有公平的机会抢到锁？

- 性能（performance）：使用锁之后增加的时间开销

## 28.5 控制中断

最早提供的互斥解决方案之一，就是在临界区关闭中断。这个解决方案是为单处理器系统开发的。

在进入临界区之前关闭中断（使用特殊的硬件指令），可以保证临界区的代码不会被中断，从而原子的执行。结束之后，我们重新打开中断，程序正常运行。

这种方法的优点是简单。但缺点很多：

- 要求我们允许所有调用线程执行特权操作，可能会被滥用。

- 不支持多处理器。

- 关闭中断导致中断丢失。

- 效率低。CPU对于关闭和打开中断的代码执行的较慢。

## 28.6 测试并设置指令（原子交换）

最简单的硬件支持是测试并设置指令（test-and-set instruction），也叫作原子交换（atomic exchange）。

先实现一个不依赖测试并设置指令的锁，用一个变量标记锁是否被持有。想法很简单：用一个变量来标志锁是否被某些线程占用。第一个线程进入临界区，调用lock()，检查标志是否为1，然后设置标志为1，表明该线程持有该锁。结束临界区时，线程调用unlock()，清除标志，表示锁未被持有。

```c
typedef struct lock_t {
    int flag;
} lock_t;


void init(lock_t *mutex) {
    // 0表示锁可用，1表示锁被某个线程持有
    mutex->flag = 0;
}


void lock(lock_t *mutex) {
    // 测试锁的flag是否可用，为1表示锁不可用，需要等待
    while (mutex->flag == 1) {
        ; // 自旋等待
    }

    // 锁可用，这时候设置flag为1，表示锁被当前线程占用
    mutex->flag = 1;
}


void unlock(lock_t *mutex) {
    // flag设置为0，表示释放锁
    mutex->flag = 0;
}
```

当第一个线程正处于临界区时，如果另一个线程调用lock()，它会在while循环中自旋等待，直到第一个线程调用unlock()清空标志。然后等待的线程会退出while循环，设置标志，执行临界区代码。

这段代码有两个问题：

- 正确性：通过中断，很容易构造出两个线程都将标志设置为1，都能进入临界区的场景。

- 性能：主要是线程在等待已经被持有的锁时，采用了自旋等待的技术。自旋等待会浪费时间。

## 29.7 实现可用的自旋锁

在SPARC上叫做ldstub（load/store unsigned byte，加载/保存无符号字节）；在x86上，是xchg（atomic exchange，原子交换）。通常称为测试并设置指令（test-and-set）：

```c
int TestAndSet(int *old_ptr, int new) {
    int old = *old_ptr; //  获取在old_ptr位置处的旧值
    *old_ptr = new; // 将old_ptr位置设置为新值
    return old; // 返回旧值
}
```

测试并设置指令做了下述事情：它返回old_ptr指向的旧值，同时更新为new的新值。这些代码是原子的执行的。因为既可以测试旧值，又可以设置新值，所以把这条指令叫做测试并设置。

利用测试并设置指令可以实现一个简单的自旋锁（spin lock）：

```c
typedef struct lock_t {
    int flag;
} lock_t;


void init(lock_t *lock) {
    // 0表示锁可用，1表示锁被某个线程持有
    lock->flag = 0;
}


void lock(lock_t *lock) {
    while (TestAndSet(&lock->flag, 1) == 1) {
        ; // 自旋等待
    }
}

void unlock(lock_t *lock) {
    lock->flag = 0;
}
```

首先假设一个线程在运行，调用lock()，没有其他线程持有锁，所以flag是0。当调用TestAndSet(flag, 1)方法，返回0，线程会跳出while循环，获取锁。同时也会原子的设置flag为1，标志锁已经被持有。当线程离开临界区，调用unlock()将flag清理为0。

第二种场景是，当某一个线程已经持有锁（即flag为1）。本线程调用lock()，然后调用TestAndSet(flag, 1)，这一次返回1。只要另一个线程一直持有锁，TestAndSet()会重复返回1，本线程会一直自旋。当flag终于被改为0，本线程会调用TestAndSet()，返回0并且原子的设置为1，从而获得锁，进入临界区。

自旋锁（spin lock）是最简单的一种锁，一直自旋，利用CPU周期，直到锁可用。

## 28.8 评价自旋锁

- 正确性：能够实现互斥，自旋锁一次只允许一个线程进入临界区

- 公平性：自旋锁不提供任何公平性保证。实际上，自旋的线程在竞态条件下可能会永远自旋。自旋锁没有公平性，可能会导致饿死。

- 性能：单CPU开销大；多CPU性能不错。

## 28.9 比较并交换

硬件原语，比较并交换指令，SPARC系统中是compare-and-swap，在x86系统中是compare-and-exchange：

```c
int CompareAndSwap(int *ptr, int expected, int new) {
    int actual = *ptr;
    if (actual == expected) {
        *ptr = new;
    }
    return actual;
}
```

比较并交换的基本思路是检测ptr指向的值是否和expected相等，如果是，更新ptr所指的值为新值；否则，什么也不做。不论哪种情况，都返回该内存地址的实际值，让调用者能够知道执行是否成功。

使用比较并交换指令，可以实现自旋锁：

```c
void lock(lock_t *lock) {
    while (CompareAndSwap(&lock->flag, 0, 1) == 1) {
        ; // 自旋
    }
}
```

检查标志是否为0，如果是，原子的交换为1，从而获得锁。锁被持有时，竞争锁的线程会自旋。

## 28.10 链接的加载和条件式存储指令

一些平台提供了实现临界区的一对指令。例如MIPS架构中，链接的加载（load-linked）和条件式存储（store-conditional）可以用来配合使用，实现其他并发结构。

伪代码：

```c
int LoadLinked(int *ptr) {
    return *ptr;
}


int StoreConditional(int *ptr, int value) {
    if (*ptr位置在LoadLinked之后没有被更新过) {
        *ptr = vaule;
        return 1; // 成功
    } else {
        return 0; 失败
    }
}
```

链接的加载是从内存中取出值存入一个寄存器。关键区别来自条件式存储指令，只有上一次加载的地址在期间都没有更新时，才会成功。成功时，条件存储返回1，并更新值；失败时，返回0，并且不会更新值。

使用链接的加载和条件式存储来实现一个锁：

```c
void lock(lock_t *lock) {
    while (1) {
        while (LoadLinked(&lock->flag) == 1) {
            ; // 自旋，直到为0
        }

        if (StoreConditional(&lock->flag, 1) == 1) {
            return;
        }
    }
}


void unlock(lock_t *lock) {
    lock->flag = 0;
}
```

## 28.11 获取并增加

获取并增加（fetch-and-add），它能原子的返回特定地址的旧值，并且让该值自增1。伪代码如下：

```c
int FetchAndAdd(int *ptr) {
    int old = *ptr;
    *ptr = old + 1;
    return old;
}
```

可以使用获取并增加指令实现一个ticket锁：

```c
typedef struct lock_t {
    int ticket;
    int turn;
} lock_t;

void lock_init(lock_t *lock) {
    lock->ticket = 0;
    lock->turn = 0;
}


void lock(lock_t *lock) {
    int myturn = FetchAndAdd(&lock->ticket);
    while (lock->turn != myturn) {
        ; // 自旋
    }
}

void unlock(lock_t *lock) {
    FetchAndAdd(&lock->turn);
}
```

基本操作：如果线程希望获取锁，首先对一个ticket值执行一个原子的获取并相加指令。这个值作为该线程的turn（顺位）。根据全局共享的lock->turn变量，当一个线程的myturn==turn时，则轮到这个线程进入临界区。unlock则是增加turn，从而下一个等待线程可以进入临界区。

本方法能够保证所有线程都能抢到锁，只要一个线程获得了ticket值，它最终会被调度。

## 28.12 自旋过多：怎么办

基于硬件的锁简单而且有效，但是某些场景下这些解决方案会效率低下。

自旋等待会浪费CPU。

## 28.13 简单方法：让出来吧，宝贝

问题：如果临界区的线程发生上下文切换，其他线程只能一直自旋，等待被中断的（持有锁的）进程重新运行。

一种简单友好的方法是，在要自旋的时候，放弃CPU：

```c
void init() {
    flag = 0;
}

void lock() {
    while (TestAndSet(&flag, 1) == 1) {
        yield(); // 放弃CPU
    }
}

void unlock() {
    flag = 0;
}
```

会有频繁的上下文切换，还会出现饥饿现象。

## 28.14 使用队列：休眠代替自旋

如果调度不合理，线程或者一直自旋，或者立刻让出CPU，无论哪种方法都可能造成浪费，也能出现饿死。

必须显式的施加某种控制，决定锁释放时，谁能抢到锁。需要操作系统的支持，并需要一个队列来保存等待锁的线程。

Solaris提供两个调用：

- park()：让调用线程休眠

- unpark(thread ID)：会唤醒thread ID标识的线程

```c
typedef struct lock_t {
    int flag;
    int guard;
    queue_t *q;
} lock_t;

void lock_init(lock_t *m) {
    m->flag = 0;
    m->guard = 0;
    queue_init(m->q);
}

void lock(lock_t *m) {
    while (TestAndSet(&m->guard, 1) == 1) {
        ; // 自旋
    }
    
    if (m->flag == 0) {
        m->flag = 1; // 获取到了锁
        m->guard = 0;
    } else {
        queue_add(m->q, gettid());
        m->guard = 0;
        park();
    }
}

void unlock(lock_t *m) {
    while (TestAndSet(&m->guard, 1) == 1) {
        ; // 自旋
    }
    
    if (queue_empty(m->q)) {
        m->flag = 0; // 队列为空，直接释放锁，没有其他线程想要获取锁
    } else {
        unpark(queue_remove(m->q));
    }
    
    m->guard = 0;
}
```



如果一个线程将要park，假定它应该睡到锁可用时。这时切换到另一个线程，这可能会导致麻烦，如果如果该线程随后释放了锁，接下来第一个线程的park会永远睡下去。这种问题称为唤醒/等待竞争（wakeup/waiting race）问题。

Solaris增加了setpark()调用来解决这个问题，一个线程表明自己马上要park，如果刚好另一个线程被调度，并且调用了unpark，那么后续的park调用就会直接返回，而不是一直睡眠。

上述代码中的lock可以修改为：

```c
queue_add(m->q, gettid());
setpark();
m->guard = 0;
```



## 28.16 两阶段锁

两阶段锁（two-phase lock），如果第一个自旋阶段没有获得锁，第二阶段调用者会睡眠，直到锁可用。

# 第29章 基于锁的并发数据结构

## 29.1 并发计数器

一个非并发的计数器：

```c
typedef struct counter_t {
    int value;
} counter_t;

void init(counter_t *c) {
    c->value = 0;
}

void increment(counter_t *c) {
    c->value++;
}

void decrement(counter_t *c) {
    c->value--;
}

int get(counter_t *c) {
    return c->value;
}
```

非并发的计数器很简单。下面是一个线程安全的计数器：

```c
typedef struct counter_t {
    int value;
    pthread_mutex_t lock;
} counter_t;

void init(counter_t *c) {
    c->value = 0;
    Pthread_mutex_init(&c->lock, NULL);
}

void increment(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    c->value++;
    Pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    c->value--;
    Pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c) {
    Pthread_mutex_lock(&c->lock);
    int rc = c->value;
    Pthread_mutex_unlock(&c->lock);
    return rc;
}
```

### 可扩展的计数

懒惰计数器（sloppy counter），通过多个局部计数器和一个全局计数器来实现一个逻辑计数器，其中每个CPU核心有一个局部计数器。每个局部计数器有一个锁，全局计数器有一个锁。

基本思想：如果一个核心上的线程想增加计数器，那就增加它的局部计数器，访问这个局部计数器是通过对应的局部锁同步的。因为每个CPU有自己的局部计数器，不同CPU上的线程不会竞争，所以计数器的更新操作可扩展性好。

但是，为了保持全局计数器更新，局部值会定期转移给全局计数器，方法是获取全局锁，让全局计数器加上局部计数器的值，然后将局部计数器置零。

```c
typedef struct counter_t {
    int global; // 全局计数
    pthread_mutex_t glock; // 全局计数锁
    int local[NUMCPUS]; // 局部计数，每个CPU一个
    pthread_mutex_t llock[NUMCPUS]; // 局部计数锁
    int threshold; // 更新频率
} counter_t;

void init(counter_t *c, int threshold) {
    c->threshold = threshold;
    c->global = 0;
    pthread_mutex_init(&c->glock, NULL);
    
    int i;
    for (i = 0; i < NUMCPUS; i++) {
        c->local[i] = 0;
        pthread_mutex_init(&c->llock[i], NULL);
    }
}

void update(counter_t *c, int threadID, int amt) {
    pthread_mutex_lock(&c->llock[threadID]);
    c->local[threadID] += amt;
    
    if (c->local[threadID] >= c->threshold) {
        pthread_mutex_lock(&c->glock);
        c->global += c->local[threadID];
        pthread_mutex_unlock(&c->glock);
        c->local[threadID] = 0;
    }
    pthread_mutex_unlock(&c->llock[threadID])
}

int get(counter_t *c) {
    pthread_mutex_lock(&c->glock);
    int val = c->global;
    pthread_mutex_unlock(&c->glock);
    return val;
}
```

## 29.2 并发链表

并发链表基础实现：

```c
typedef struct node_t {
    int key;
    struct node_t *next;
} node_t;

typedef struct list_t {
    node_t *head;
    pthread_mutex_t lock;
} list_t;

void List_Init(list_t *L) {
    L->head = NULL;
    pthread_mutex_init(&L->lock, NULL);
}

void List_Insert(list_t *L, int key) {
    node_t *new = malloc(sizeof(node_t));
    if (new == NULL) {
        return ;
    }
    new->key = key;

    pthread_mutex_lock(&L->lock);
    new->next = L->head;
    L->head = new;
    pthread_mutex_unlock(&L->lock);
}

int List_Lookup(list_t *L, int key) {
    pthread_mutex_lock(&L->lock);
    node_t *curr = L->head;
    while (curr) {
        if (curr->key == key) {
            pthread_mutex_unlock(&L->lock);
            return 0;
        }
        curr = curr->next;
    }
    pthread_mutex_unlock(&L->lock);
    return -1;
}
```

## 29.3 并发队列

```c
typedef struct __node_t {
    int value;
    struct __node_t *next;
} node_t;

typedef struct queue_t {
    node_t *head;
    node_t *tail;
    pthread_mutex_t headLock;
    pthread_mutex_t tailLock;
} queue_t;

void Queue_Init(queue_t *q) {
    node_t *tmp = malloc(sizeof(node_t));
    tmp->next = NULL;
    q->head = q->tail = tmp;
    pthread_mutex_init(&q->headLock, NULL);
    pthread_mutex_init(&q->tailLock, NULL);
}

void Queue_Enqueue(queue_t *q, int value) {
    node_t *tmp = malloc(sizeof(node_t));
    assert(tmp != NULL);
    tmp->value = value;
    tmp->next = NULL;

    pthread_mutex_lock(&q->tailLock);
    q->tail->next = tmp;
    q->tail = tmp;
    pthread_mutex_unlock(&q->tailLock);
}

int Queue_Dequeue(queue_t *q, int *value) {
    pthread_mutex_lock(&q->headLock);
    node_t *tmp = q->head;
    node_t *newHead = tmp->next;
    if (newHead == NULL) {
        pthread_mutex_unlock(&q->headLock);
        return -1;
    }
    *value = newHead->value;
    q->head = newHead;
    pthread_mutex_unlock(&q->headLock);
    free(tmp);
    return 0;
}
```

## 29.4 并发散列表

```c
#define BUCKETS (101)

typedef struct __hash_t {
    list_t lists[BUCKETS];
} hash_t;

void Hash_Init(hast_t *H) {
    int i;
    for (i = 0; i < BUCKETS; i++) {
        List_Init(&H->lists[i]);
    }
}

int Hash_Insert(hash_t *H, int key) {
    int bucket = key % BUCKETS;
    return List_Insert(&H->lists[bucket], key);
}

int Hask_Lookup(hash_t *H, int key) {
    int bucket = key % BUCKETS;
    return List_Lookup(&H->lists[bucket], key);
}
```

# 第30章 条件变量

锁并不是并发程序设计所需的唯一原语。在很多情况下，线程需要检查某一条件（condition）满足之后，才会继续运行。

## 30.1 定义和程序

线程可以使用条件变量（condition variable），来等待一个条件变成真。条件变量是一个显式队列，当某些执行状态（即条件，condition）不满足时，线程可以把自己加入队列，等待（waiting）该条件。另外某个线程，当他改变了上述状态时，就可以唤醒一个或者多个等待线程（通过在该条件上发信号），让他们继续执行。

声明条件变量：`pthread_cond_t c`，条件变量有两种相关操作：wait()线程睡眠和signal()唤醒等待在某个条件变量上的睡眠线程。

## 30.2 生产者/消费者（有界缓冲区）问题

生产者/消费者（consumer/producer）问题，也叫作有界缓冲区（bounded buffer）问题。

Mesa语义。

Hoare语义。

```c
int buffer[MAX];
int fill = 0;
int use = 0;
int count = 0;

void put(int value) {
    buffer[fill] = value;
    fill = (fill + 1) % MAX;
    count++;
}

int get() {
    int tmp = buffer[use];
    use = (use + 1) % MAX;
    count--;
    return tmp;
}

cond_t empty, fill;
mutex_t mutex;

void *producer(void *arg) {
    int i;
    for (i = 0; i < loops; i++) {
        Pthread_mutex_lock(&mutex);
        while (count == MAX) {
            Pthread_cond_wait(&empty, &mutex);
        }
        put(i);
        Pthread_cond_signal(&fill);
        Pthread_mutex_unlock(&mutex);
    }
}

void *consumer(void *arg) {
    int i;
    for (i = 0; i < loops; i++) {
        Pthread_mutex_lock(&mutex);
        while (count == 0) {
            Pthread_cond_wait(&fill, &mutex);
        }
        int tmp = get();
        Pthread_cond_signal(&empty);
        Pthread_mutex_unlock(&mutex);
        printf("%d\n", tmp);
    }
}
```

## 30.3 覆盖条件

能覆盖所有需要唤醒的线程的场景，这种条件变量叫做覆盖条件（covering condition）。